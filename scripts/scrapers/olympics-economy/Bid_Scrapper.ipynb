{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping - Olympics Bidding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code scrapes the olympics bidding data from wikipedia for summer and winter games. The data consists of the number of participants in each bidding year, number of times the country won or lost the bid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympics_year_bid_data = {'Host_Year': None, 'Bid_Year': None, 'Bid_City': None, 'Bid_Country': None}\n",
    "country_bid_data = {'Country': None, 'City': None, 'Failed_Bids': None, 'Success_Bids': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Downloading Web page\n",
    "def download_page(web_url, name):\n",
    "    downloaded_page = requests.get(web_url)\n",
    "\n",
    "    # Saving the webpage as .html file\n",
    "    file_obj = open(name, 'wb')\n",
    "    file_obj.write(downloaded_page.content)\n",
    "    file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Utils Function\n",
    "def check_dash_cell(cellvalue):\n",
    "    if cellvalue == \"â€”\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Returns the year from string date\n",
    "def extract_years(cellvalue):\n",
    "    if not check_dash_cell(cellvalue):\n",
    "        years_list = [int(s) for s in re.findall(r'-?\\d+\\.?\\d*', cellvalue.text.strip())]\n",
    "        return years_list if len(years_list) > 0 else list()\n",
    "    return list()\n",
    "\n",
    "def populate_data(row_data, index_list):\n",
    "    if row_data[index_list[0]].a is None:\n",
    "        country_bid_data['City'] = row_data[index_list[0]].text \n",
    "    else:\n",
    "        country_bid_data['City'] = row_data[index_list[0]].a.text\n",
    "    country_bid_data['Failed_Bids'] = extract_years(row_data[index_list[1]])\n",
    "    country_bid_data['Success_Bids'] = extract_years(row_data[index_list[2]])\n",
    "    \n",
    "def extract_location(row_data, index_list):\n",
    "    # Bidding City\n",
    "    if row_data[index_list[0]].a is None:\n",
    "        olympics_year_bid_data['Bid_City'] = row_data[index_list[0]].text \n",
    "    else:\n",
    "        olympics_year_bid_data['Bid_City'] = row_data[index_list[0]].a.text \n",
    "\n",
    "    # Bidding Country\n",
    "    a_list = row_data[index_list[1]].find_all(\"a\")\n",
    "    if len(a_list) > 1:\n",
    "        olympics_year_bid_data['Bid_Country'] = a_list[1].text\n",
    "    else:\n",
    "        olympics_year_bid_data['Bid_Country'] = a_list[0].text\n",
    "\n",
    "def get_bid_year(input_value):\n",
    "    return input_value.split(\" \")[-1] if not check_dash_cell(input_value) else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(url, file_name, title, start_index):\n",
    "\n",
    "    # Additional Variables\n",
    "    bid_by_year_df = pd.DataFrame(columns=['Host_Year', 'Bid_Year', 'Bid_City', 'Bid_Country'])\n",
    "    bid_by_country_df = pd.DataFrame(columns=['Country', 'City', 'Failed_Bids', 'Success_Bids'])\n",
    "\n",
    "    download_page(url, file_name)\n",
    "\n",
    "    # Scrapes the data and saves it as .csv file\n",
    "    with open(file_name, 'rb') as page:\n",
    "        content = page.read()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        tables_list = soup.find_all('table', attrs={'class': 'wikitable'})\n",
    "\n",
    "        # Bids by Year\n",
    "        tab_one_rows = tables_list[0].find_all(\"tr\")\n",
    "        for rowvalue in tab_one_rows[start_index:]:\n",
    "\n",
    "            row_data = rowvalue.find_all(\"td\")\n",
    "            no_cols = len(row_data)\n",
    "\n",
    "            first_column = row_data[0].a.text if row_data[0].a is not None else row_data[0].text\n",
    "            \n",
    "            if no_cols >= 2 and re.match(r'^([\\s\\d]+)$', first_column):\n",
    "                # Host Year\n",
    "                olympics_year_bid_data['Host_Year'] = first_column\n",
    "                \n",
    "                # Bidding Year\n",
    "                if row_data[1].a is None:\n",
    "                    olympics_year_bid_data['Bid_Year'] = get_bid_year(row_data[1].text)\n",
    "                else:\n",
    "                    olympics_year_bid_data['Bid_Year'] = get_bid_year(row_data[1].a.text)\n",
    "\n",
    "                # Bidding City and Country\n",
    "                if row_data[2].a is None and row_data[3].a is None:\n",
    "                    olympics_year_bid_data['Bid_City'] = row_data[2].text if not check_dash_cell(row_data[2]) else \"\"\n",
    "                    olympics_year_bid_data['Bid_Country'] = row_data[3].text if not check_dash_cell(row_data[3]) else \"\"\n",
    "\n",
    "                else:\n",
    "                    extract_location(row_data, [2, 3])\n",
    "\n",
    "                bid_by_year_df = bid_by_year_df.append(olympics_year_bid_data, ignore_index=True)\n",
    "\n",
    "            elif no_cols>=3:\n",
    "                extract_location(row_data, [0, 1])\n",
    "                if row_data[2].i is None or (row_data[2].i is not None and not row_data[2].i.text == 'Withdrew'):\n",
    "                    bid_by_year_df = bid_by_year_df.append(olympics_year_bid_data, ignore_index=True)\n",
    "\n",
    "            elif no_cols >= 2:\n",
    "                extract_location(row_data, [0, 1])\n",
    "                bid_by_year_df = bid_by_year_df.append(olympics_year_bid_data, ignore_index=True)\n",
    "\n",
    "        # Bids by Country\n",
    "        tab_two_rows = tables_list[1].find_all(\"tr\")\n",
    "        for rowvalue in tab_two_rows[1:]:\n",
    "            row_data = rowvalue.find_all(\"td\")\n",
    "            no_cols = len(row_data)\n",
    "            if no_cols == 3:\n",
    "                populate_data(row_data, [0, 1, 2])\n",
    "\n",
    "            if no_cols == 4:\n",
    "                country_bid_data['Country'] = row_data[0].a['title']\n",
    "                populate_data(row_data, [1, 2, 3])\n",
    "\n",
    "            bid_by_country_df = bid_by_country_df.append(country_bid_data, ignore_index=True)\n",
    "\n",
    "    # Writing the Structured data to csv format\n",
    "    bid_by_year_df.to_csv('Datasets/'+ title + 'bidbyyear.csv', index=False)\n",
    "    bid_by_country_df.to_csv('Datasets/'+ title + 'bidbycountry.csv', index=False)\n",
    "\n",
    "# Main method\n",
    "if __name__ == '__main__':\n",
    "    scrape_data(\"https://en.wikipedia.org/wiki/List_of_bids_for_the_Summer_Olympics\", \"summer_bids.html\",'summer', 4)\n",
    "    scrape_data(\"https://en.wikipedia.org/wiki/List_of_bids_for_the_Winter_Olympics\", \"winter_bids.html\",'winter', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda",
   "language": "python",
   "name": "tf-gpu-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
