{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision Of Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the World Bank API to access the world bank data \n",
    "# !pip install wbgapi\n",
    "\n",
    "import wbgapi as wb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features or Columns for the model\n",
    "cols = ['t-4', 't-3', 't-2', 't-1', 't0', 'host', 'impact']\n",
    "\n",
    "# Dataframe to save the prepared input data for the model\n",
    "model_data_df = None\n",
    "\n",
    "# Selected Economic Indicators \n",
    "economic_variables = {'NY.GDP.PCAP.CD':'GDP Per Capita', 'ST.INT.ARVL':'Tourism Arrival', 'NY.GDP.DEFL.KD.ZG':'Inflation',\n",
    "                      'PA.NUS.FCRF':'Exchange Rate', 'GC.DOD.TOTL.GD.ZS':'Debt', 'NY.GDP.MKTP.KD.ZG':'GDP Growth',\n",
    "                      'NE.TRD.GNFS.ZS':'Trade', 'NE.IMP.GNFS.ZS':'Import Goods', 'GC.TAX.TOTL.GD.ZS':'Tax Revenue',\n",
    "                      'CM.MKT.TRAD.GD.ZS':'Stocks', 'BX.KLT.DINV.WD.GD.ZS':'Foreign Investments',\n",
    "                      'NE.DAB.TOTL.ZS': 'Gross National Expenditure', 'GC.REV.XGRT.GD.ZS':'Revenue',\n",
    "                      'SL.IND.EMPL.ZS':'Employment', 'SL.UEM.TOTL.NE.ZS':'Unemployment'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Modify this code\n",
    "alias_names = {'GER':'DEU', 'FRG':'DEU', 'NED':'NLD', 'SUI':'CHE', 'YUG':'SRB', 'URS':'RUS', 'GRE':'GRC'}\n",
    "\n",
    "year_list = [i for i in range(1960, 2021)]\n",
    "\n",
    "summer_olympics_data = pd.read_csv(\"Datasets/summer.csv\") \n",
    "winter_olympics_data = pd.read_csv(\"Datasets/winter.csv\")\n",
    "\n",
    "summer_olympics_data = summer_olympics_data.replace(alias_names)\n",
    "winter_olympics_data = winter_olympics_data.replace(alias_names)\n",
    "\n",
    "winter_olympics_data.dropna(inplace=True)\n",
    "summer_olympics_data.dropna(inplace=True)\n",
    "\n",
    "summer_host_info = summer_olympics_data.groupby('country_code')['year'].apply(list).reset_index().explode('year')\n",
    "winter_host_info = winter_olympics_data.groupby('country_code')['year'].apply(list).reset_index().explode('year')\n",
    "\n",
    "summer_host_info = summer_host_info[(summer_host_info['year']>=1964) & (summer_host_info['year'] <= 2016)]\n",
    "winter_host_info = winter_host_info[(winter_host_info['year']>=1964) & (winter_host_info['year'] <= 2016)]\n",
    "\n",
    "games_df = pd.concat([summer_host_info, winter_host_info], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to create Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the economic data of olympic host countries\n",
    "def populate_host_data(eco_df, games_df, host_value):\n",
    "    \n",
    "    host_list = eco_df.index\n",
    "    \n",
    "    for row in games_df.itertuples(index=False):\n",
    "        row_values = []\n",
    "        global model_data_df\n",
    "        \n",
    "        if row.country_code in host_list:\n",
    "            host_year_value = eco_df.loc[row.country_code, row.year]\n",
    "            before_values = eco_df.loc[row.country_code, [i for i in range(row.year-4, row.year)]].to_list()\n",
    "            for i in before_values:\n",
    "                row_values.append(i) \n",
    "            row_values.append(host_year_value)\n",
    "            row_values.append(host_value)\n",
    "            after_effects = eco_df.loc[row.country_code, [j for j in range(row.year+1, row.year+5)]]\n",
    "\n",
    "            if np.average(after_effects) >= host_year_value:\n",
    "                row_values.append(np.max(after_effects))\n",
    "            else:\n",
    "                row_values.append(np.min(after_effects))\n",
    "            \n",
    "            model_data_df = model_data_df.append(pd.Series(row_values, index = model_data_df.columns), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the economic data of olympic non-host countries\n",
    "def populate_non_host_data(eco_df, year_list, host_value):\n",
    "    \n",
    "    def get_impact_value(rowdata):\n",
    "        yrs_list = rowdata[:-1].to_list()\n",
    "        mean_value = rowdata['mean']\n",
    "        if mean_value >= yrs_list[0]:\n",
    "            return max(yrs_list)\n",
    "        else:\n",
    "            return min(yrs_list)\n",
    "    \n",
    "    for yr in year_list:\n",
    "        \n",
    "        global model_data_df\n",
    "        \n",
    "        before_df = pd.DataFrame(columns=cols)\n",
    "        \n",
    "        before_df[cols[:4]] = eco_df[[i for i in range(yr-4, yr)]]\n",
    "        after_df = eco_df[[i for i in range(yr, yr+5)]]\n",
    "        \n",
    "        after_df['mean'] = after_df.mean(axis=1)\n",
    "        after_df['max_impact'] = after_df.apply(get_impact_value, axis=1)\n",
    "        \n",
    "        before_df[cols[4]] = eco_df[yr]\n",
    "        before_df[cols[5]] = host_value\n",
    "        before_df[cols[6]] = after_df['max_impact']\n",
    "\n",
    "        model_data_df = pd.concat([model_data_df, before_df], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the indicator data for all countries\n",
    "def get_indicator_data(indicator):\n",
    "    eco_df = wb.data.DataFrame(indicator, skipAggs=True)\n",
    "    eco_df = eco_df.rename(columns=lambda x: int(x.replace('YR', '')))\n",
    "    eco_df = eco_df.transpose().fillna(method='backfill').fillna(method='ffill').transpose()\n",
    "    eco_df.dropna(inplace=True)\n",
    "    return eco_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression\n",
    "2. Random Forest Regressor\n",
    "3. Suppor Vector Regressor\n",
    "4. Gradient Boosting Regressor\n",
    "5. Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(data, cols):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[cols[0:6]], data[cols[6]], test_size=0.3, random_state=42)\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(inputdata):\n",
    "    # Scale the data before applying the model\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(inputdata)\n",
    "    inputdata = scaler.transform(inputdata)\n",
    "    return inputdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_score(input_data_df, cols):\n",
    "    \n",
    "    # Scale the data before applying the model\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(input_data_df[cols[0:6]])\n",
    "    input_data_df[cols[0:6]] = scaler.transform(input_data_df[cols[0:6]])\n",
    "    \n",
    "    print('Training Linear Regression ...')\n",
    "\n",
    "    # Splitting the data set\n",
    "    X_train, X_test, y_train, y_test = get_train_test_split(input_data_df, cols)\n",
    "    \n",
    "    # Training different Linear Regression model\n",
    "    \n",
    "    # Vanilla Linear Regression\n",
    "    lr_model = LinearRegression().fit(X_train, y_train)\n",
    "    \n",
    "    # Ridge Regression\n",
    "    #lr_model = linear_model.Ridge(alpha=1.0).fit(X_train, np.log(y_train))\n",
    "    \n",
    "    # Lasso Regression\n",
    "    #lr_model = linear_model.Lasso(alpha=1.0).fit(X_train, np.log(y_train))\n",
    "    \n",
    "    # Bayesian Ridge Regression\n",
    "    #lr_model = linear_model.BayesianRidge().fit(X_train, y_train)\n",
    "    \n",
    "    # Gamma Regressor\n",
    "    #lr_model = linear_model.GammaRegressor().fit(X_train, y_train)\n",
    "    \n",
    "    # Tweedie Regression\n",
    "    #lr_model = linear_model.TweedieRegressor(power=2, alpha=1.0, link='log').fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and return the score\n",
    "    # User to inverse log transform\n",
    "    #pred = np.exp(lr_model.predict(X_test))\n",
    "    pred = lr_model.predict(X_test)\n",
    "\n",
    "    print(\"MSE : \", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "    return lr_model, np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf_score(input_data_df, cols):\n",
    "    \n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [10, 20, 30, 40, 50],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200]\n",
    "    }\n",
    "\n",
    "    print('Training Random Forest Regression ...')\n",
    "    \n",
    "    # Splitting the data set\n",
    "    X_train, X_test, y_train, y_test = get_train_test_split(input_data_df, cols)\n",
    "    \n",
    "    # Hyperparameter Tuning\n",
    "    grid_search = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Training the model\n",
    "    rf_model = RandomForestRegressor(n_estimators = best_params['n_estimators'], \n",
    "                                     min_samples_split = best_params['min_samples_split'], \n",
    "                                     min_samples_leaf= best_params['min_samples_leaf'],\n",
    "                                     max_features = best_params['max_features'], \n",
    "                                     max_depth= best_params['max_depth'], bootstrap=best_params['bootstrap'])\n",
    "\n",
    "    #rf_model = RandomForestRegressor(n_estimators = 100, max_depth= 5, bootstrap=True)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Prediction\n",
    "    pred = rf_model.predict(X_test)\n",
    "    \n",
    "    print(\"MSE : \", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "    return rf_model, np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svr_score(input_data_df, cols):\n",
    "    \n",
    "    # Scale the data before applying the model\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(input_data_df[cols[0:6]])\n",
    "    input_data_df[cols[0:6]] = scaler.transform(input_data_df[cols[0:6]])\n",
    "    \n",
    "    param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                  'kernel': ['rbf', 'poly']}\n",
    " \n",
    "    print('Training Support Vector Regression ...')\n",
    "\n",
    "    # Splitting the data set\n",
    "    X_train, X_test, y_train, y_test = get_train_test_split(input_data_df, cols)\n",
    "    \n",
    "    # Hyperparameter Tuning\n",
    "    grid_search = GridSearchCV(SVR(), param_grid, refit = True, verbose = 1, cv=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Training the model\n",
    "    svr_model = SVR(C=best_params['C'], gamma=best_params['gamma'], kernel=best_params['kernel'])\n",
    "    svr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prediction\n",
    "    pred = svr_model.predict(X_test)\n",
    "    \n",
    "    print(\"MSE : \", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "    \n",
    "    return svr_model, np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_score(input_data_df, cols):\n",
    "    parameters = { 'loss' : ['ls', 'lad', 'huber', 'quantile'],\n",
    "                   'learning_rate' : (0.05,0.25,0.50,1),\n",
    "                   'criterion' : ['friedman_mse', 'mse', 'mae'],\n",
    "                   'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                   'n_estimators': [100, 200, 500],\n",
    "                   'max_depth': [1, 2],\n",
    "                   'min_samples_leaf': [5,10],\n",
    "                   'min_samples_split': [5,10]\n",
    "                }\n",
    "    \n",
    "    print('Training Gradient Boosting Regressor ...')\n",
    "    \n",
    "    # Splitting the data set\n",
    "    X_train, X_test, y_train, y_test = get_train_test_split(input_data_df, cols)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Ignoring # Hyperparameter tuning because it is computationally expensive.\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(GradientBoostingRegressor(), parameters, cv=3, verbose=2)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    best_params = model.best_params_\n",
    "        \n",
    "    # Training the model\n",
    "    xgb_model = GradientBoostingRegressor(loss=best_params['loss'], learning_rate=best_params['learning_rate'],\n",
    "                                    criterion=best_params['criterion'], max_features=best_params['max_features'],\n",
    "                                    n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'],\n",
    "                                    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                    min_samples_split=best_params['min_samples_split']).fit(X_train, y_train)\n",
    "    '''\n",
    "    \n",
    "    xgb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=10, loss='squared_error').fit(X_train, y_train)\n",
    "    \n",
    "    # Prediction\n",
    "    pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    print(\"MSE : \", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "    \n",
    "    return xgb_model, np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_score(input_data_df, cols):\n",
    "    \n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': [(20,20),(10,),],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.1, 0.05],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "    \n",
    "    print('Training Multi Layer Perceptron ...')\n",
    "    \n",
    "     # Scale the data before applying the model\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(input_data_df[cols[0:6]])\n",
    "    input_data_df[cols[0:6]] = scaler.transform(input_data_df[cols[0:6]])\n",
    "    \n",
    "    # Splitting the data set\n",
    "    X_train, X_test, y_train, y_test = get_train_test_split(input_data_df, cols)\n",
    "    \n",
    "    # Hyperparameter Tuning\n",
    "    grid_search = GridSearchCV(MLPRegressor(random_state=1, max_iter=50), parameter_space, n_jobs=-1, cv=5, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    " \n",
    "    # Training the model\n",
    "    mlp_model = MLPRegressor(max_iter=50, \n",
    "                        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "                        activation=best_params['activation'],\n",
    "                        solver=best_params['solver'],\n",
    "                        alpha=best_params['alpha'],\n",
    "                        learning_rate=best_params['learning_rate']).fit(X_train, y_train)\n",
    "    \n",
    "    # Prediction\n",
    "    pred = mlp_model.predict(X_test)\n",
    "    \n",
    "    print(\"MSE : \", np.sqrt(mean_squared_error(y_test, pred)))\n",
    "    \n",
    "    return mlp_model, np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model for each Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './testmodel/'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for key, value in economic_variables.items():\n",
    "    \n",
    "    print(\"-----------------------\"+ key + \":\" + value +\"--------------------------\")\n",
    "\n",
    "    global model_data_df\n",
    "    model_data_df = pd.DataFrame(columns=cols)\n",
    "    eco_df = get_indicator_data(key)\n",
    "    \n",
    "    saved_models = {'lr':None, 'rf':None, 'svr':None, 'xgb':None, 'mlp':None}\n",
    "    model_performance = {'lr':None, 'rf':None, 'svr':None, 'xgb':None, 'mlp':None}\n",
    "\n",
    "    host_country_df = eco_df[eco_df.index.isin(games_df.country_code.unique())]\n",
    "    populate_host_data(host_country_df, games_df, 1)\n",
    "\n",
    "    nonhost_country_df = eco_df[~eco_df.index.isin(games_df.country_code.unique())]\n",
    "    populate_non_host_data(nonhost_country_df, games_df.year.to_list(), 0)\n",
    "    \n",
    "    model_data_df[cols] = np.cbrt(model_data_df[cols])\n",
    "\n",
    "    print(\"Model Training and Hyperparameter Tuning...\")\n",
    "\n",
    "    saved_models['lr'], model_performance['lr'] = get_lr_score(model_data_df, cols)\n",
    "#     saved_models['rf'], model_performance['rf'] = get_rf_score(model_data_df, cols)\n",
    "#     saved_models['svr'], model_performance['svr'] = get_svr_score(model_data_df, cols)\n",
    "#     saved_models['xgb'], model_performance['xgb'] = get_xgb_score(model_data_df, cols)\n",
    "#     saved_models['mlp'], model_performance['mlp'] = get_mlp_score(model_data_df, cols)\n",
    "\n",
    "#     print(\"Linear Regression : \", model_performance['lr'])\n",
    "#     print(\"Random Forest : \", model_performance['rf'])\n",
    "#     print(\"Support Vector Regression : \" ,model_performance['svr'])\n",
    "#     print(\"Gradient Boosting Regression : \" ,model_performance['xgb'])\n",
    "#     print(\"Multi Layer Perceptron : \" ,model_performance['mlp'])\n",
    "\n",
    "#     max_key = max(model_performance, key=model_performance.get)\n",
    "#     print(\"Model Performance : \", model_performance['lr'])\n",
    "    \n",
    "    # Saving the model\n",
    "    filename = path + value + '.pkl'\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(saved_models['lr'], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Adding 2019 and 2020 actual values to the forecasted data.\n",
    "for key, value in economic_variables.items():\n",
    "    print(\"running...\")\n",
    "    eco_df = get_indicator_data(key)\n",
    "    forecast_data = pd.read_csv(value + \".csv\")\n",
    "    forecast_data[str(2020)] = eco_df[2020].values\n",
    "    forecast_data[str(2019)] = eco_df[2019].values\n",
    "    forecast_data.to_csv(value + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for key, value in economic_variables.items():    \n",
    "    loaded_model = pickle.load(open('./testmodel/' + value + '.pkl', 'rb'))\n",
    "    test_data = pd.read_csv('./time_series/'+ value + '.csv').set_index('Unnamed: 0')\n",
    "    yrs = ['2024','2025','2026','2027','2028']\n",
    "    \n",
    "    # Cube root Transformation\n",
    "    test_data[test_data.columns] = np.cbrt(test_data[test_data.columns])\n",
    "\n",
    "    # Standard Scaling\n",
    "    test_data = test_data.transpose()\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(test_data)\n",
    "    test_data.loc[:] = scaler.transform(test_data)\n",
    "    test_data = test_data.transpose()\n",
    "\n",
    "    inputdata = test_data.loc[:, yrs]\n",
    "    inputdata['host'] = 1\n",
    "\n",
    "    input_row = inputdata.loc['AUS'].values.reshape(1,-1)\n",
    "    predicted_value = loaded_model.predict(input_row)\n",
    "    print(\"Predicted \" + value + \" : \",predicted_value**3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda",
   "language": "python",
   "name": "tf-gpu-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}